
import requests
from bs4 import BeautifulSoup
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re
from typing import List, Dict, Tuple

class WebContentExtractor:
    """Pipeline estruturado para extraÃ§Ã£o e anÃ¡lise de conteÃºdo web."""
    
    def __init__(self):
        # Inicializa o modelo de embeddings
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
    def scrape_content(self, url: str) -> str:
        """Extrai conteÃºdo limpo de uma URL."""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Remove elementos irrelevantes
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 
                               'aside', 'iframe', 'noscript']):
                element.decompose()
            
            # Extrai texto principal
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
            if main_content:
                text = main_content.get_text()
            else:
                text = soup.get_text()
            
            # Limpa o texto
            text = re.sub(r'\s+', ' ', text).strip()
            return text
            
        except Exception as e:
            return f"Erro ao extrair conteÃºdo: {str(e)}"
    
    def chunk_text(self, text: str, chunk_size: int = 300) -> List[str]:
        """Divide o texto em chunks menores."""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            if len(chunk.strip()) > 50:  # Ignora chunks muito pequenos
                chunks.append(chunk.strip())
        
        return chunks
    
    def find_relevant_chunk(self, chunks: List[str], query: str) -> Tuple[str, float]:
        """Encontra o chunk mais relevante usando busca semÃ¢ntica."""
        if not chunks:
            return "", 0.0
        
        # Gera embeddings
        query_embedding = self.embedding_model.encode([query])
        chunk_embeddings = self.embedding_model.encode(chunks)
        
        # Calcula similaridade
        similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]
        
        # Encontra o chunk mais similar
        best_idx = np.argmax(similarities)
        best_chunk = chunks[best_idx]
        best_score = similarities[best_idx]
        
        return best_chunk, best_score
    
    def extract_focused_content(self, url: str, query: str, min_similarity: float = 0.3) -> Dict:
        """Pipeline completo: scraping -> chunking -> busca semÃ¢ntica."""
        print(f"ğŸ” Extraindo conteÃºdo de: {url}")
        
        # 1. Scraping
        content = self.scrape_content(url)
        if content.startswith("Erro"):
            return {"error": content}
        
        # 2. Chunking
        chunks = self.chunk_text(content)
        print(f"ğŸ“„ Dividido em {len(chunks)} chunks")
        
        # 3. Busca semÃ¢ntica
        relevant_chunk, similarity = self.find_relevant_chunk(chunks, query)
        
        if similarity < min_similarity:
            return {
                "error": f"Nenhum conteÃºdo relevante encontrado (similaridade: {similarity:.2f})",
                "similarity": similarity
            }
        
        return {
            "success": True,
            "relevant_content": relevant_chunk,
            "similarity": similarity,
            "source_url": url,
            "total_chunks": len(chunks)
        }

def generate_maintenance_response(content_data: Dict, original_query: str) -> str:
    """Gera resposta focada usando apenas o conteÃºdo extraÃ­do."""
    if "error" in content_data:
        return f"âŒ {content_data['error']}"
    
    relevant_content = content_data["relevant_content"]
    source_url = content_data["source_url"]
    similarity = content_data["similarity"]
    
    # Template de resposta estruturada
    response = f"""ğŸ”§ **Resposta baseada em fonte confiÃ¡vel:**

{relevant_content}

ğŸ“Š **AnÃ¡lise da informaÃ§Ã£o:**
â€¢ RelevÃ¢ncia: {similarity:.1%}
â€¢ Fonte verificada: âœ…
â€¢ Contexto: ManutenÃ§Ã£o Industrial

ğŸ”— **ReferÃªncia:** {source_url}

ğŸ’¡ **AplicaÃ§Ã£o prÃ¡tica:** Esta informaÃ§Ã£o pode ser aplicada diretamente em procedimentos de manutenÃ§Ã£o preditiva e corretiva."""
    
    return response

# FunÃ§Ã£o principal para integraÃ§Ã£o com a aplicaÃ§Ã£o existente
def analyze_web_content_for_maintenance(query: str, urls: List[str]) -> str:
    """Analisa conteÃºdo web com foco em manutenÃ§Ã£o industrial."""
    extractor = WebContentExtractor()
    
    best_result = None
    best_similarity = 0
    
    for url in urls:
        try:
            result = extractor.extract_focused_content(url, query)
            if result.get("success") and result.get("similarity", 0) > best_similarity:
                best_result = result
                best_similarity = result["similarity"]
        except Exception as e:
            continue
    
    if best_result:
        return generate_maintenance_response(best_result, query)
    else:
        return """âŒ **Nenhum conteÃºdo relevante encontrado**

ğŸ”„ **SugestÃµes:**
â€¢ Reformule a pergunta com termos mais especÃ­ficos
â€¢ Verifique se as URLs estÃ£o acessÃ­veis
â€¢ Tente incluir palavras-chave tÃ©cnicas da Ã¡rea de manutenÃ§Ã£o

ğŸ’¬ **Alternativa:** Descreva sua dÃºvida especÃ­fica sobre manutenÃ§Ã£o industrial."""
